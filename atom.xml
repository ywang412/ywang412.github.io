<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://ywang412.github.io</id>
    <title>Yu&apos;s Github</title>
    <updated>2019-12-15T22:59:21.222Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://ywang412.github.io"/>
    <link rel="self" href="https://ywang412.github.io/atom.xml"/>
    <subtitle>Java, SQL, Python and a little bit of Scala </subtitle>
    <logo>https://ywang412.github.io/images/avatar.png</logo>
    <icon>https://ywang412.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, Yu&apos;s Github</rights>
    <entry>
        <title type="html"><![CDATA[Java design pattern]]></title>
        <id>https://ywang412.github.io/post/java-design-pattern</id>
        <link href="https://ywang412.github.io/post/java-design-pattern">
        </link>
        <updated>2019-12-14T13:35:48.000Z</updated>
        <content type="html"><![CDATA[<p><img src="https://ywang412.github.io/post-images/1576381012394.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1576381064677.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1576381098946.png" alt=""></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spring MVC]]></title>
        <id>https://ywang412.github.io/post/spring-mvc</id>
        <link href="https://ywang412.github.io/post/spring-mvc">
        </link>
        <updated>2019-12-13T23:32:12.000Z</updated>
        <content type="html"><![CDATA[<figure data-type="image" tabindex="1"><img src="https://ywang412.github.io/post-images/1576380797947.png" alt=""></figure>
<figure data-type="image" tabindex="2"><img src="https://ywang412.github.io/post-images/1576380877003.png" alt=""></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spring transaction]]></title>
        <id>https://ywang412.github.io/post/spring-transaction</id>
        <link href="https://ywang412.github.io/post/spring-transaction">
        </link>
        <updated>2019-12-11T16:09:35.000Z</updated>
        <content type="html"><![CDATA[<p>spring transaction can be implemented by</p>
<ol>
<li>
<p>TransactionTemplate<br>
Service DI transactionTemplate<br>
TransactionTemplate DI TransactionManager<br>
TransactionManager DI datasource / Set autocommit false</p>
</li>
<li>
<p>XML<br>
1.TransactionProxyFactoryBean DI service and TransactionManager (PROPAGATION_REQUIRED, +exception)  one ProxyFactoryBean per service class<br>
2.AspectJ<br>
single point / txadvice DI TransactionManager (tx:method propagation isolation)<br>
multiple points / aop: pointcut expression<br>
3. @Transactional(propagation, isolation, readOnly, rollbackfor exceptions, noRollbackFor)<br>
&lt;tx:annotation-driven transaction-manager&gt;</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[NoSQL vs SQL]]></title>
        <id>https://ywang412.github.io/post/nosql-vs-sql</id>
        <link href="https://ywang412.github.io/post/nosql-vs-sql">
        </link>
        <updated>2019-11-04T13:25:02.000Z</updated>
        <content type="html"><![CDATA[<figure data-type="image" tabindex="1"><img src="https://ywang412.github.io/post-images/1576386520494.png" alt=""></figure>
<p><strong>RDMBS</strong></p>
<p>Benefit</p>
<ul>
<li>SQL</li>
<li>Joins</li>
<li>Aggregation</li>
<li>good for small data volume</li>
<li>Secondary index</li>
<li>model data independent of Queries</li>
</ul>
<p>Drawback</p>
<ul>
<li>only scale vertically</li>
<li>schema not flexible</li>
</ul>
<p>Normalization  - reduce redundency and increase correctness<br>
denormalization - increase performance for read heavy</p>
<p><strong>Cassendra</strong></p>
<ul>
<li>table - group of partition</li>
<li>Partition - collection of rows - unit of access</li>
<li>PK - partition key (Sharding) + clustering columns (sorting within partition desc)</li>
<li>Cassandra Collection: Set, List, Map</li>
</ul>
<p>Good for</p>
<ul>
<li>logging events</li>
<li>IOT</li>
<li>time series db</li>
<li>heavy write<br>
Bad for</li>
<li>ad - hoc queries</li>
<li>joins</li>
</ul>
<p>Denormalization is a must for cassendra / model queries no joins /  one query per table</p>
<p><strong>MongoDB</strong></p>
<p>Embedded and Referenced Relationships</p>
<pre><code>Manual References
{
   &quot;_id&quot;:ObjectId(&quot;52ffc33cd85242f436000001&quot;),
   &quot;contact&quot;: &quot;987654321&quot;,
   &quot;dob&quot;: &quot;01-01-1991&quot;,
   &quot;name&quot;: &quot;Tom Benzamin&quot;,
   &quot;address_ids&quot;: [
      ObjectId(&quot;52ffc4a5d85242602e000000&quot;),
      ObjectId(&quot;52ffc4a5d85242602e000001&quot;)
   ]
}
&gt;var result = db.users.findOne({&quot;name&quot;:&quot;Tom Benzamin&quot;},{&quot;address_ids&quot;:1})
&gt;var addresses = db.address.find({&quot;_id&quot;:{&quot;$in&quot;:result[&quot;address_ids&quot;]}})

DBRefs
{
   &quot;_id&quot;:ObjectId(&quot;53402597d852426020000002&quot;),
   &quot;address&quot;: {
   &quot;$ref&quot;: &quot;address_home&quot;,
   &quot;$id&quot;: ObjectId(&quot;534009e4d852427820000002&quot;),
   &quot;$db&quot;: &quot;tutorialspoint&quot;},
   &quot;contact&quot;: &quot;987654321&quot;,
   &quot;dob&quot;: &quot;01-01-1991&quot;,
   &quot;name&quot;: &quot;Tom Benzamin&quot;
}
</code></pre>
<p>Covered Queries</p>
<pre><code>&gt;db.users.ensureIndex({gender:1,user_name:1})
Covered Query (fetch the required data from indexed data which is very fast. not go looking into database documents.)
&gt;db.users.find({gender:&quot;M&quot;},{user_name:1,_id:0}
Not Covered Query (index does not include _id field)
&gt;db.users.find({gender:&quot;M&quot;},{user_name:1})
</code></pre>
<p>Atomic Operations</p>
<pre><code>&gt;db.products.findAndModify({ 
   query:{_id:2,product_available:{$gt:0}}, 
   update:{ 
      $inc:{product_available:-1}, 
      $push:{product_bought_by:{customer:&quot;rob&quot;,date:&quot;9-Jan-2014&quot;}} 
   }    
})
</code></pre>
<p>Indexing Array Fields and Sub-Document Fields</p>
<p>An ObjectId is a 12-byte BSON type having the following structure −<br>
The first 4 bytes representing the seconds since the unix epoch<br>
The next 3 bytes are the machine identifier<br>
The next 2 bytes consists of process id<br>
The last 3 bytes are a random counter value</p>
<p>Text search</p>
<pre><code>&gt;db.adminCommand({setParameter:true,textSearchEnabled:true})
&gt;db.posts.ensureIndex({post_text:&quot;text&quot;})
&gt;db.posts.find({$text:{$search:&quot;tutorialspoint&quot;}})
</code></pre>
<p>Auto-Increment Sequence</p>
<pre><code>&gt;function getNextSequenceValue(sequenceName){

   var sequenceDocument = db.counters.findAndModify({
      query:{_id: sequenceName },
      update: {$inc:{sequence_value:1}},
      new:true
   });
	
   return sequenceDocument.sequence_value;
}
</code></pre>
<p><strong>Mongodb diagnosis and optimization</strong></p>
<p>web service response time &lt; 200ms<br>
mongodb response time &lt; 100ms<br>
long response time</p>
<ol>
<li>proper index use explain()</li>
<li>cacheSizeGB ram size use mongostat()<br>
connection fail</li>
<li>maxIncomingConnections db.serverStatus().connections shows available connections</li>
<li>ulimit -a -&gt; open files -&gt; max file descriptors</li>
</ol>
<p><strong>AWS redshift RDS table design optimization</strong></p>
<ul>
<li>
<p>Distribution style<br>
Even - The leader node distributes the rows across the slices in a round-robin fashion<br>
Auto - Amazon Redshift assigns an optimal distribution style based on the size of the table data<br>
Key - The leader node places matching values on the same node slice<br>
ALL - replicate table on all nodes</p>
</li>
<li>
<p>Sorting Key<br>
define a colum as sort key</p>
</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://ywang412.github.io/post-images/1573580426980.png" alt=""></figure>
<pre><code>CREATE TABLE part (
  p_partkey     	integer     	not null	sortkey distkey,
  p_name        	varchar(22) 	not null,
  p_mfgr        	varchar(6)      not null,
  p_category    	varchar(7)      not null,
  p_brand1      	varchar(9)      not null,
  p_color       	varchar(11) 	not null,
  p_type        	varchar(25) 	not null,
  p_size        	integer     	not null,
  p_container   	varchar(10)     not null
);
</code></pre>
<p>Distribution key and sort key significantly improve query time</p>
<p><strong>Neo4J</strong></p>
<pre><code>// Friend-of-a-friend 
(user)-[:KNOWS]-(friend)-[:KNOWS]-(foaf)
// Shortest path
path = shortestPath( (user)-[:KNOWS*..5]-(other) )
// Collaborative filtering
(user)-[:PURCHASED]-&gt;(product)&lt;-[:PURCHASED]-()-[:PURCHASED]-&gt;(otherProduct)
// Tree navigation 
(root)&lt;-[:PARENT*]-(leaf:Category)-[:ITEM]-&gt;(data:Product)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SQL cheatsheet]]></title>
        <id>https://ywang412.github.io/post/sql-cheatsheet</id>
        <link href="https://ywang412.github.io/post/sql-cheatsheet">
        </link>
        <updated>2019-10-01T11:24:27.000Z</updated>
        <content type="html"><![CDATA[<p><strong>time function</strong></p>
<pre><code>WHERE t.login_date BETWEEN DATE_SUB('2019-06-30', interval 90 day) AND '2019-06-30' 
WHERE datediff( '2019-06-30', t.login_date)&lt;=90
</code></pre>
<p><strong>SQL window function</strong></p>
<pre><code>SELECT start_terminal,
       duration_seconds,
       SUM(duration_seconds) OVER
         (PARTITION BY start_terminal ORDER BY start_time)
         AS running_total,
       COUNT(duration_seconds) OVER
         (PARTITION BY start_terminal ORDER BY start_time)
         AS running_count,
       AVG(duration_seconds) OVER
         (PARTITION BY start_terminal ORDER BY start_time)
         AS running_avg
			 ROW_NUMBER() OVER (PARTITION BY start_terminal
									ORDER BY start_time)
						AS row_numbe
			-- RANK() would give the identical rows and then skip
			-- DENSE_RANK() would give the identical rows and then no-skip
			 RANK() OVER (PARTITION BY start_terminal  
							ORDER BY start_time)
				AS rank
  FROM tutorial.dc_bikeshare_q1_2012
 WHERE start_time &lt; '2012-01-08'
</code></pre>
<p>LAG pulls from previous rows and LEAD pulls from following rows</p>
<pre><code>SELECT *
  FROM (
    SELECT start_terminal,
           duration_seconds,
           duration_seconds -LAG(duration_seconds, 1) OVER
             (PARTITION BY start_terminal ORDER BY duration_seconds)
             AS difference
      FROM tutorial.dc_bikeshare_q1_2012
     WHERE start_time &lt; '2012-01-08'
     ORDER BY start_terminal, duration_seconds
       ) sub
 WHERE sub.difference IS NOT NULL
</code></pre>
<p><strong>534. Game Play Analysis III</strong></p>
<pre><code>--Method 1: Using Self Join 
SELECT a1.player_id, a1.event_date,
SUM(a2.games_played) AS games_played_so_far
FROM activity a1, activity a2
WHERE a1.player_id = a2.player_id
AND a1.event_date &gt;=a2.event_date
GROUP BY a1.player_id, a1.event_date
ORDER BY a1.player_id, a1.event_date;
--Method 2: Using window functions

SELECT
player_id, event_date, sum(games_played) over(PARTITION BY player_id ORDER BY event_date)
AS 'games_played_so_far'
FROM activity
ORDER BY player_id, games_played_so_far;
</code></pre>
<p><strong>601. Human Traffic Of Stadium</strong></p>
<pre><code>select distinct day1.* 
from 
stadium day1, stadium day2, stadium day3
where 
day1.people &gt;= 100 and day2.people &gt;= 100 and day3.people &gt;= 100 and
((day1.id + 1 =  day2.id and day1.id + 2 = day3.id) or 
(day1.id - 1 = day2.id and day1.id + 1 = day3.id) or 
(day1.id - 2 = day2.id and day1.id - 1 = day3.id)) 
order by day1.id; 

select day1.* 
from 
stadium day1, stadium day2, stadium day3
where 
day1.people &gt;= 100 and day2.people &gt;= 100 and day3.people &gt;= 100 and
((day1.id + 1 =  day2.id and day1.id + 2 = day3.id) or 
(day1.id - 1 = day2.id and day1.id + 1 = day3.id) or 
(day1.id - 2 = day2.id and day1.id - 1 = day3.id)) 
GROUP BY day1.id; 
</code></pre>
<p><strong>612. Shortest Distance in a Plane</strong></p>
<pre><code>SELECT
    round(min(sqrt(pow(P1.y-P2.y,2)+pow(P1.x-P2.x,2))),2) as shortest
FROM
    point_2d P1,
    point_2d P2
WHERE
    P1.x &lt;&gt; P2.x OR
    P1.y &lt;&gt; P2.y
</code></pre>
<p><strong>Delete duplicate emails</strong></p>
<pre><code>DELETE p1
FROM Person p1, Person p2
WHERE p1.Email = p2.Email AND
p1.Id &gt; p2.Id

Delete From Person
Where Id not in (
    Select minId From (
        Select min(Id) AS minId, Email
        From Person
        Group by Email
    ) AS tmp
);
</code></pre>
<p><strong>Rank scores</strong></p>
<p>join vs subquery</p>
<pre><code>SELECT Score, 
(SELECT COUNT(DISTINCT Score) FROM Scores WHERE Score &gt;= s.Score) Rank 
FROM Scores s ORDER BY Score DESC;

SELECT s.Score, COUNT(DISTINCT t.Score) Rank
FROM Scores s JOIN Scores t ON s.Score &lt;= t.Score
GROUP BY s.Id ORDER BY s.Score DESC;
--Group by s.Id so COUNT is per id

Select Score, Rank From (
    Select Id, Score,
        @rank := if(@preScore = Score, @rank, @rank+1) AS Rank,
        @preScore := Score AS preScore
    From Scores, (Select @rank:=0, @preScore:=-1) var
    Order by Score DESC
    ) tmp;
</code></pre>
<p><strong>Exchange seat</strong></p>
<p>if clause</p>
<pre><code>select
if(id &lt; (select count(*) from seat), if(id mod 2=0, id-1, id+1), if(id mod 2=0, id-1, id)) as id, student
from seat
order by id asc;

SELECT (CASE 
    WHEN mod(id, 2) != 0 and records != id THEN id + 1
    WHEN mod(id, 2) != 0 and records = id THEN id
    ELSE id - 1
END) AS id, student
FROM seat, (select count(*) as records from seat) as seat_records 
-- as seat_records because Every derived table must have its own alias
ORDER BY id asc;
</code></pre>
<p><strong>610. Triangle Judgement</strong></p>
<pre><code>SELECT 
    x,
    y,
    z,
    CASE
        WHEN x + y &gt; z AND x + z &gt; y AND y + z &gt; x THEN 'Yes'
        ELSE 'No'
    END AS 'triangle'
FROM
    triangle;

select *, 
    IF(x + y &gt; z AND x + z &gt; y AND y + z &gt; x, 'Yes', 'No') as triangle 
    from triangle;

</code></pre>
<p><strong>Friend requests I</strong></p>
<p>round + ifnull</p>
<pre><code>select
round(
    ifnull(
    (select count(*) from (select distinct requester_id, accepter_id from request_accepted) as A)
    /
    (select count(*) from (select distinct sender_id, send_to_id from friend_request) as B),
    0)
, 2) as accept_rate;

select ifnull(round((count(distinct requester_id,accepter_id)/count(distinct sender_id,send_to_id)),2),0.00) as accept_rate
from friend_request, request_accepted
</code></pre>
<p><strong>Friend requests II</strong></p>
<p>union all</p>
<pre><code>SELECT ids as id , COUNT(*) as num
FROM 
  (SELECT requester_id as ids FROM request_accepted
   UNION ALL
   SELECT accepter_id as ids FROM request_accepted) AS u
GROUP BY ids
ORDER BY COUNT(*) DESC
LIMIT 1
</code></pre>
<p><strong>Team Scores in Football Tournament</strong></p>
<p>Left join on or</p>
<pre><code>SELECT team_id,team_name,
SUM(CASE WHEN team_id=host_team AND host_goals&gt;guest_goals THEN 3 ELSE 0 END)+
SUM(CASE WHEN team_id=guest_team AND guest_goals&gt;host_goals THEN 3 ELSE 0 END)+
SUM(CASE WHEN team_id=host_team AND host_goals=guest_goals THEN 1 ELSE 0 END)+
SUM(CASE WHEN team_id=guest_team AND guest_goals=host_goals THEN 1 ELSE 0 END)
as num_points
FROM Teams
LEFT JOIN Matches
ON team_id=host_team OR team_id=guest_team
GROUP BY team_id
ORDER BY num_points DESC, team_id ASC;

select t.team_id, t.team_name, 
        sum(case 
            when T.host_goals &gt; T.guest_goals then 3 
            when T.host_goals = T.guest_goals then 1
            else 0
            end) as num_points
from teams t left join
(select * from matches
union all
select match_id, guest_team, host_team, guest_goals, host_goals from matches) as T on t.team_id = T.host_team
group by t.team_id
order by num_points desc, t.team_id asc
</code></pre>
<p><strong>Monthly Transactions II</strong></p>
<pre><code>SELECT month, country, 

SUM(IF( state = &quot;approved&quot;,1,0)) AS approved_count,
SUM(CASE WHEN state = &quot;approved&quot; THEN amount ELSE 0 END) AS approved_amount, 
SUM(IF(state='back',1,0)) AS chargeback_count,
SUM(CASE WHEN state = &quot;back&quot; THEN amount ELSE 0 END) AS chargeback_amount
FROM
(
   SELECT DATE_FORMAT(c.trans_date, '%Y-%m') AS month, country,   -- LEFT(trans_date, 7)
   &quot;back&quot; AS state, amount
   FROM chargebacks AS c
   JOIN transactions AS t ON c.trans_id = t.id
   UNION ALL
   
   SELECT DATE_FORMAT(t.trans_date, '%Y-%m') AS month, country, state, amount
   FROM transactions AS t
   WHERE state = &quot;approved&quot;
) s
GROUP BY month, country
</code></pre>
<p><strong>Students Report By Geography</strong></p>
<p>@variable</p>
<pre><code>SELECT 
    America, Asia, Europe
FROM
    (SELECT @as:=0, @am:=0, @eu:=0) t,
    (SELECT 
        @as:=@as + 1 AS asid, name AS Asia
    FROM
        student
    WHERE
        continent = 'Asia'
    ORDER BY Asia) AS t1
        RIGHT JOIN
    (SELECT 
        @am:=@am + 1 AS amid, name AS America
    FROM
        student
    WHERE
        continent = 'America'
    ORDER BY America) AS t2 ON asid = amid
        LEFT JOIN
    (SELECT 
        @eu:=@eu + 1 AS euid, name AS Europe
    FROM
        student
    WHERE
        continent = 'Europe'
    ORDER BY Europe) AS t3 ON amid = euid
;
</code></pre>
<p><strong>571. Find Median Given Frequency of Numbers</strong></p>
<pre><code>select  avg(n.Number) median
from Numbers n
where n.Frequency &gt;= abs((select sum(Frequency) from Numbers where Number&lt;=n.Number) -
                         (select sum(Frequency) from Numbers where Number&gt;=n.Number))
												 
select avg(t3.Number) as median
from Numbers as t3 
inner join 
    (select t1.Number, 
        abs(sum(case when t1.Number&gt;t2.Number then t2.Frequency else 0 end) -
            sum(case when t1.Number&lt;t2.Number then t2.Frequency else 0 end)) as count_diff
    from numbers as t1, numbers as t2
    group by t1.Number) as t4
on t3.Number = t4.Number
where t3.Frequency&gt;=t4.count_diff
</code></pre>
<p><strong>603. Consecutive Available Seats</strong></p>
<pre><code>select distinct a.seat_id
from cinema a join cinema b
  on abs(a.seat_id - b.seat_id) = 1
  and a.free = true and b.free = true
order by a.seat_id
;
</code></pre>
<p><strong>184. Department Highest Salary</strong></p>
<pre><code>SELECT D.Name AS Department ,E.Name AS Employee ,E.Salary 
FROM
	Employee E,
	(SELECT DepartmentId,max(Salary) as max FROM Employee GROUP BY DepartmentId) T,
	Department D
WHERE E.DepartmentId = T.DepartmentId 
  AND E.Salary = T.max
  AND E.DepartmentId = D.id
  
SELECT D.Name AS Department ,E.Name AS Employee ,E.Salary 
from 
	Employee E,
	Department D 
WHERE E.DepartmentId = D.id 
  AND (DepartmentId,Salary) in 
  (SELECT DepartmentId,max(Salary) as max FROM Employee GROUP BY DepartmentId)
</code></pre>
<p><strong>185. Department Top Three Salaries</strong></p>
<pre><code>select d.Name Department, e1.Name Employee, e1.Salary
from Employee e1 
join Department d
on e1.DepartmentId = d.Id
where 3 &gt; (select count(distinct(e2.Salary)) 
                  from Employee e2 
                  where e2.Salary &gt; e1.Salary 
                  and e1.DepartmentId = e2.DepartmentId
                  );
									
Select D.Name AS Department, T.Name AS Employee, T.Salary
From 
(
Select DepartmentId, Name, Salary,
    (CASE WHEN @id=DepartmentId THEN @rank:= IF(@preSalary = Salary, @rank, @rank+1)
         ELSE @rank:= 1
    END) AS Rank,
    @preSalary:= Salary AS preSalary,
    @id:= DepartmentId AS preId
From Employee, (Select @rank:=0, @preSalary:=-1, @id:=NULL) var
Order by DepartmentId, Salary DESC
) T JOIN Department D
ON T.DepartmentId = D.Id
Where T.Rank &lt;=3;
</code></pre>
<p><strong>176. Second Highest Salary</strong></p>
<pre><code>CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT
BEGIN
  RETURN (
      SELECT MAX(Salary) FROM Employee E1
      WHERE N - 1 =
      (SELECT COUNT(DISTINCT(E2.Salary)) FROM Employee E2
      WHERE E2.Salary &gt; E1.Salary)
  );
END

SELECT max(Salary)
FROM Employee
WHERE Salary &lt; (SELECT max(Salary) FROM Employee)
</code></pre>
<p><strong>Nth Highest Salary</strong></p>
<pre><code>SELECT Salary FROM Employee GROUP BY Salary
UNION ALL (SELECT NULL AS Salary)
ORDER BY Salary DESC LIMIT 1 OFFSET 1;
 

SELECT MAX(Salary) FROM Employee E1
WHERE N =
(SELECT COUNT(DISTINCT(E2.Salary)) FROM Employee E2
WHERE E2.Salary &gt;= E1.Salary);

Select Distinct Salary From (
    Select Id, Salary,
        @rank := if(@preSalary = Salary, @rank, @rank+1) AS Rank,
        @preSalary := Salary AS preSalary
    From Employee, (Select @rank:=0, @preSalary:=-1) var
    Order by Salary DESC
    ) tmp
Where Rank=N;
</code></pre>
<p><strong>1204. Last Person to Fit in the Elevator</strong></p>
<pre><code>SELECT q1.person_name
FROM Queue q1 JOIN Queue q2 ON q1.turn &gt;= q2.turn
GROUP BY q1.turn
HAVING SUM(q2.weight) &lt;= 1000
ORDER BY SUM(q2.weight) DESC
LIMIT 1

SELECT person_name FROM Queue as a
WHERE
(
    SELECT SUM(weight) FROM Queue as b
    WHERE b.turn&lt;=a.turn
    ORDER By turn
)&lt;=1000
ORDER BY a.turn DESC limit 1;
</code></pre>
<p><strong>NOT EXISTS vs. NOT IN vs. LEFT JOIN WHERE IS NULL</strong></p>
<p>NOT IN never matches if there is a single NULL in the list.</p>
<pre><code>SELECT a FROM table1 WHERE a NOT IN (SELECT a FROM table2)
SELECT a FROM table1 WHERE NOT EXISTS (SELECT * FROM table2 WHERE table1.a = table2.a)
SELECT a FROM table1 LEFT JOIN table2 ON table1.a = table2.a WHERE table1.a IS NULL

DELETE b FROM BLOB b 
  LEFT JOIN FILES f ON f.id = b.fileid 
      WHERE f.id IS NULL

DELETE FROM BLOB 
 WHERE NOT EXISTS(SELECT NULL
                    FROM FILES f
                   WHERE f.id = fileid)
									 
DELETE FROM BLOB
 WHERE fileid NOT IN (SELECT f.id 
                        FROM FILES f)
</code></pre>
<p><strong>Find Local Maximum</strong></p>
<pre><code>SELECT b.index
FROM points AS a, points AS b, points AS c
WHERE  a.index = b.index-1 AND c.index = b.index+1
AND a.value &lt; b.value   AND c.value &lt; b.value 
	 
select ind
from tmp1 t1
where val &gt; (select val from jdoyle.tmp1 t2 where t2.ind = t1.ind-1)
and val &gt; (select val from jdoyle.tmp1 t2 where t2.ind = t1.ind+1);
</code></pre>
<p><strong>Consecutive Numbers</strong></p>
<pre><code>Select Distinct Num As ConsecutiveNums From (
    Select Id, Num, 
        @count := If(@preNum = Num, @count+1, 1) AS Count,
        @preNum := Num AS preNum
    From Logs, (Select @preNum:=NULL, @count:=1) var 
    ) tmp
Where Count &gt;=3;
</code></pre>
<p><strong>197. Rising Temperature</strong></p>
<pre><code>Select W1.Id
From Weather W1 JOIN Weather W2 
ON to_days(W1.Date) = to_days(W2.Date) + 1 --DATEDIFF(a.Date, b.Date)=1
Where W1.Temperature &gt; W2.Temperature;

Select Id From (
Select Id, Date, Temperature,
    @Higher := If(Temperature &gt; @preTemp, 'Yes', 'No') AS Higher,
    @DateContinuous := If(Datediff(Date, @preDate) = 1, 'Yes', 'No') AS DateContinuous,
    @preTemp := Temperature AS preTemp,
    @preDate := Date
From Weather, (Select @preTemp:=NULL, @Higher:=NULL, @preDate:=NULL) var
Order by Date
) tmp
Where Higher = 'Yes' AND DateContinuous = 'Yes';
</code></pre>
<p><strong>262 Trips and Users</strong></p>
<pre><code>SELECT t.Request_at as Day,
       ROUND(COUNT(IF(Status &lt;&gt; 'completed', TRUE, NULL)) / COUNT(*), 2) AS 'Cancellation Rate' 
from Trips t 
inner join Users u 
on t.Client_Id = u.Users_Id and u.Banned='No'
and t.Request_at between '2013-10-01' and '2013-10-03'
group by t.Request_at

SELECT request_at AS &quot;Day&quot;, 
            ROUND(((SUM(CASE WHEN LOWER(Status) LIKE &quot;cancelled%&quot; THEN 1.000 ELSE 0 END)) / COUNT(id)), 2) AS &quot;Cancellation Rate&quot; 
FROM        trips
WHERE       client_id NOT IN (SELECT users_id FROM users WHERE banned = 'Yes')
AND         request_at BETWEEN '2013-10-01' AND '2013-10-03'
GROUP BY    request_at;
</code></pre>
<p><strong>182. Select duplicated email</strong></p>
<pre><code>select Email
from Person
group by Email
having count(Email) &gt; 1;
</code></pre>
<p><strong>627. Swap Salary</strong></p>
<pre><code>UPDATE salary
SET
    sex = CASE sex
        WHEN 'm' THEN 'f'
        ELSE 'm'
    END;
</code></pre>
<p><strong>585. Investments in 2016</strong></p>
<pre><code>select sum(TIV_2016) TIV_2016
from insurance
where TIV_2015 in (select TIV_2015 from insurance group by TIV_2015 having count(1)&gt;1)
and (lat, lon) in (select lat, lon from insurance group by lat, lon having count(1)=1)
</code></pre>
<p><strong>569. Median Employee Salary</strong></p>
<pre><code>SELECT 
    Id, Company, Salary
FROM
    (SELECT 
        e.Id,
            e.Salary,
            e.Company,
            IF(@prev = e.Company, @Rank:=@Rank + 1, @Rank:=1) AS rank,
            @prev:=e.Company
    FROM
        Employee e, (SELECT @Rank:=0, @prev:=0) AS temp
    ORDER BY e.Company , e.Salary , e.Id) Ranking
        INNER JOIN
    (SELECT 
        COUNT(*) AS totalcount, Company AS name
    FROM
        Employee e2
    GROUP BY e2.Company) companycount ON companycount.name = Ranking.Company
WHERE
    Rank = FLOOR((totalcount + 1) / 2)
        OR Rank = FLOOR((totalcount + 2) / 2)
</code></pre>
<p><strong>579. Find Cumulative Salary of an Employee</strong></p>
<pre><code>SELECT e.id, e.month,
    SUM(e2.Salary) as Salary
FROM Employee e
JOIN Employee e2 ON (e.Id = e2.Id AND e.Month &gt;= e2.Month AND (e.Month - e2.Month &lt;= 2))
WHERE e.Month &lt; (SELECT MAX(Month) from Employee where Id = e.Id)
GROUP BY e.Id, e.Month
ORDER BY e.Id, e.Month DESC
</code></pre>
<p><strong>608. Tree Node</strong></p>
<pre><code>SELECT
    atree.id,
    IF(ISNULL(atree.p_id),
        'Root',
        IF(atree.id IN (SELECT p_id FROM tree), 'Inner','Leaf')) Type
FROM
    tree atree
ORDER BY atree.id

select id, 
case when p_id is null then 'Root' 
else CASE WHEN EXISTS (SELECT 1 FROM tree t2 WHERE t2.p_id = t1.id)
                      THEN 'Inner'
                       ELSE 'Leaf'
                 END
    END AS Type
from tree t1 order by id;
</code></pre>
<p><strong>1084. Sales Analysis III</strong></p>
<p>Group by having</p>
<pre><code>SELECT s.product_id, product_name
FROM Sales s
LEFT JOIN Product p
ON s.product_id = p.product_id
GROUP BY s.product_id
HAVING MIN(sale_date) &gt;= CAST('2019-01-01' AS DATE) AND
       MAX(sale_date) &lt;= CAST('2019-03-31' AS DATE)

select product_id, product_name
from product
where product_id not in
(select product_id
from sales
where sale_date not between '2019-01-01' and '2019-03-31');
</code></pre>
<p><strong>IS NOT NULL</strong></p>
<pre><code>SELECT *
FROM products
WHERE category_id IS NOT NULL;
</code></pre>
<p><strong>608. Tree Node</strong></p>
<pre><code>select T.id, 
IF(isnull(T.p_id), 'Root', IF(T.id in (select p_id from tree), 'Inner', 'Leaf')) Type 
from tree T

select Id,
case 
    when p_id is null then &quot;Root&quot;
    when (p_id is not null and id in (select p_id from Tree)) then &quot;Inner&quot;
    else &quot;Leaf&quot;
end as Type
from tree

SELECT DISTINCT t1.id, (
    CASE
    WHEN t1.p_id IS NULL  THEN 'Root'
    WHEN t1.p_id IS NOT NULL AND t2.id IS NOT NULL THEN 'Inner'
    WHEN t1.p_id IS NOT NULL AND t2.id IS NULL THEN 'Leaf'
    END
) AS Type 
FROM tree t1
LEFT JOIN tree t2
ON t1.id = t2.p_id
</code></pre>
<p><strong>1082. Sales Analysis I</strong></p>
<pre><code>SELECT seller_id
FROM Sales
GROUP BY seller_id
HAVING sum(price) = 
(   SELECT sum(price)
    FROM sales
    group by seller_id
    order by sum(price) desc
    limit 1)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AWS Redshift and Apache Airflow pipeline]]></title>
        <id>https://ywang412.github.io/post/aws-redshift-and-apache-airflow-pipeline</id>
        <link href="https://ywang412.github.io/post/aws-redshift-and-apache-airflow-pipeline">
        </link>
        <updated>2019-08-19T13:10:43.000Z</updated>
        <content type="html"><![CDATA[<p>A reusable production-grade data pipeline that incorporates data quality checks and allows for easy backfills. The source data resides in S3 and needs to be processed in a data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.</p>
<ol>
<li>
<p>Create AWS redshift cluster and test queries.<br>
<img src="https://ywang412.github.io/post-images/1573755046671.png" alt=""></p>
</li>
<li>
<p>Set up AWS S3 hook<br>
<img src="https://ywang412.github.io/post-images/1573760821111.png" alt=""></p>
</li>
<li>
<p>Set up redshift connection hook<br>
<img src="https://ywang412.github.io/post-images/1573760949674.png" alt=""></p>
</li>
<li>
<p>Set up Airflow job DAG<br>
<img src="https://ywang412.github.io/post-images/1573778681268.png" alt=""></p>
</li>
<li>
<p>Run Airflow scheduler<br>
<img src="https://ywang412.github.io/post-images/1573778839068.png" alt=""></p>
</li>
<li>
<p>See past job statistics<br>
<img src="https://ywang412.github.io/post-images/1573779317276.png" alt=""></p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[AWS EMR Spark and Data Lake]]></title>
        <id>https://ywang412.github.io/post/aws-emr-spark-and-data-lake</id>
        <link href="https://ywang412.github.io/post/aws-emr-spark-and-data-lake">
        </link>
        <updated>2019-08-14T03:20:45.000Z</updated>
        <content type="html"><![CDATA[<figure data-type="image" tabindex="1"><img src="https://ywang412.github.io/post-images/1573690987278.png" alt=""></figure>
<p>An ETL pipeline that extracts data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables.</p>
<p>Create a Data Lake with Spark and AWS EMR</p>
<ol>
<li>create a ssh key-pair to securely connect to the EMR cluster</li>
<li>create an EMR cluster<br>
<img src="https://ywang412.github.io/post-images/1573691132968.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1573691150215.png" alt=""></li>
<li>ssh into the master node<br>
<img src="https://ywang412.github.io/post-images/1573691211629.png" alt=""></li>
<li>access master node jupyter notebook<br>
<img src="https://ywang412.github.io/post-images/1573691449872.png" alt=""></li>
</ol>
<pre><code class="language-python">print(&quot;Welcome to my EMR Notebook!&quot;)
</code></pre>
<pre><code>VBox()


Starting Spark application
</code></pre>
<table>
<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1573680517609_0004</td><td>pyspark</td><td>idle</td><td><a target="_blank" href="http://ip-172-31-90-61.ec2.internal:20888/proxy/application_1573680517609_0004/">Link</a></td><td><a target="_blank" href="http://ip-172-31-94-174.ec2.internal:8042/node/containerlogs/container_1573680517609_0004_01_000001/livy">Link</a></td><td>✔</td></tr></table>
<pre><code>FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


SparkSession available as 'spark'.



FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


Welcome to my EMR Notebook!
</code></pre>
<pre><code class="language-python">%%info
</code></pre>
<p>Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'}</tt><br></p>
<table>
<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1573680517609_0004</td><td>pyspark</td><td>idle</td><td><a target="_blank" href="http://ip-172-31-90-61.ec2.internal:20888/proxy/application_1573680517609_0004/">Link</a></td><td><a target="_blank" href="http://ip-172-31-94-174.ec2.internal:8042/node/containerlogs/container_1573680517609_0004_01_000001/livy">Link</a></td><td>✔</td></tr></table>
<pre><code class="language-python">sc.list_packages()
</code></pre>
<pre><code>VBox()



FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


Package                    Version
-------------------------- -------
beautifulsoup4             4.8.1  
boto                       2.49.0 
jmespath                   0.9.4  
lxml                       4.4.1  
mysqlclient                1.4.4  
nltk                       3.4.5  
nose                       1.3.4  
numpy                      1.14.5 
pip                        19.3.1 
py-dateutil                2.2    
python36-sagemaker-pyspark 1.2.6  
pytz                       2019.3 
PyYAML                     3.11   
setuptools                 41.6.0 
six                        1.12.0 
soupsieve                  1.9.4  
wheel                      0.33.6 
windmill                   1.6
</code></pre>
<pre><code class="language-python">sc.install_pypi_package(&quot;pandas==0.25.1&quot;)
</code></pre>
<pre><code>VBox()



FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


Collecting pandas==0.25.1
  Downloading https://files.pythonhosted.org/packages/73/9b/52e228545d14f14bb2a1622e225f38463c8726645165e1cb7dde95bfe6d4/pandas-0.25.1-cp36-cp36m-manylinux1_x86_64.whl (10.5MB)
Requirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib64/python3.6/site-packages (from pandas==0.25.1) (1.14.5)
Collecting python-dateutil&gt;=2.6.1
  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)
Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas==0.25.1) (2019.3)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil&gt;=2.6.1-&gt;pandas==0.25.1) (1.12.0)
Installing collected packages: python-dateutil, pandas
Successfully installed pandas-0.25.1 python-dateutil-2.8.1
</code></pre>
<pre><code class="language-python">sc.install_pypi_package(&quot;matplotlib&quot;, &quot;https://pypi.org/simple&quot;) #Install matplotlib from given PyPI repository
</code></pre>
<pre><code>VBox()



FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


Collecting matplotlib
  Downloading https://files.pythonhosted.org/packages/57/4f/dd381ecf6c6ab9bcdaa8ea912e866dedc6e696756156d8ecc087e20817e2/matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)
Requirement already satisfied: numpy&gt;=1.11 in /usr/local/lib64/python3.6/site-packages (from matplotlib) (1.14.5)
Collecting kiwisolver&gt;=1.0.1
  Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB)
Requirement already satisfied: python-dateutil&gt;=2.1 in /mnt/tmp/1573683191129-0/lib/python3.6/site-packages (from matplotlib) (2.8.1)
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1
  Downloading https://files.pythonhosted.org/packages/c0/0c/fc2e007d9a992d997f04a80125b0f183da7fb554f1de701bbb70a8e7d479/pyparsing-2.4.5-py2.py3-none-any.whl (67kB)
Collecting cycler&gt;=0.10
  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl
Requirement already satisfied: setuptools in /mnt/tmp/1573683191129-0/lib/python3.6/site-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib) (41.6.0)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil&gt;=2.1-&gt;matplotlib) (1.12.0)
Installing collected packages: kiwisolver, pyparsing, cycler, matplotlib
Successfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.1.1 pyparsing-2.4.5
</code></pre>
<pre><code class="language-python">sc.list_packages()
</code></pre>
<pre><code>VBox()



FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


Package                    Version
-------------------------- -------
beautifulsoup4             4.8.1  
boto                       2.49.0 
cycler                     0.10.0 
jmespath                   0.9.4  
kiwisolver                 1.1.0  
lxml                       4.4.1  
matplotlib                 3.1.1  
mysqlclient                1.4.4  
nltk                       3.4.5  
nose                       1.3.4  
numpy                      1.14.5 
pandas                     0.25.1 
pip                        19.3.1 
py-dateutil                2.2    
pyparsing                  2.4.5  
python-dateutil            2.8.1  
python36-sagemaker-pyspark 1.2.6  
pytz                       2019.3 
PyYAML                     3.11   
setuptools                 41.6.0 
six                        1.12.0 
soupsieve                  1.9.4  
wheel                      0.33.6 
windmill                   1.6
</code></pre>
<pre><code class="language-python">df = spark.read.parquet('s3://amazon-reviews-pds/parquet/product_category=Books/*.parquet')
</code></pre>
<pre><code>VBox()



FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…
</code></pre>
<pre><code class="language-python">df.printSchema()
num_of_books = df.select('product_id').distinct().count()
print(f'Number of Books: {num_of_books:,}')
</code></pre>
<pre><code>VBox()



FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


root
 |-- marketplace: string (nullable = true)
 |-- customer_id: string (nullable = true)
 |-- review_id: string (nullable = true)
 |-- product_id: string (nullable = true)
 |-- product_parent: string (nullable = true)
 |-- product_title: string (nullable = true)
 |-- star_rating: integer (nullable = true)
 |-- helpful_votes: integer (nullable = true)
 |-- total_votes: integer (nullable = true)
 |-- vine: string (nullable = true)
 |-- verified_purchase: string (nullable = true)
 |-- review_headline: string (nullable = true)
 |-- review_body: string (nullable = true)
 |-- review_date: date (nullable = true)
 |-- year: integer (nullable = true)

Number of Books: 3,423,743
</code></pre>
<ol start="5">
<li>install python libraries</li>
</ol>
<pre><code>sudo easy_install-3.6 pip 
sudo /usr/local/bin/pip3 install paramiko nltk scipy scikit-learn pandas
</code></pre>
<ol start="6">
<li>
<p>upload file to EMR<br>
<img src="https://ywang412.github.io/post-images/1573691565612.png" alt=""></p>
</li>
<li>
<p>spark-submit job</p>
</li>
</ol>
<pre><code class="language-python">import configparser
from datetime import datetime
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format
from pyspark.sql import functions as F
from pyspark.sql import types as T

import pandas as pd
pd.set_option('display.max_columns', 500)
</code></pre>
<pre><code class="language-python">config = configparser.ConfigParser()

#Normally this file should be in ~/.aws/credentials
config.read_file(open('dl.cfg'))

os.environ[&quot;AWS_ACCESS_KEY_ID&quot;]= config['AWS']['AWS_ACCESS_KEY_ID']
os.environ[&quot;AWS_SECRET_ACCESS_KEY&quot;]= config['AWS']['AWS_SECRET_ACCESS_KEY']
</code></pre>
<pre><code class="language-python">def create_spark_session():
    spark = SparkSession \
        .builder \
        .config(&quot;spark.jars.packages&quot;, &quot;org.apache.hadoop:hadoop-aws:2.7.0&quot;) \
        .getOrCreate()
    return spark
</code></pre>
<pre><code class="language-python"># create the spark session
spark = create_spark_session()
</code></pre>
<pre><code class="language-python"># read data from my S3 bucket. This is the same data in workspace
songPath = 's3a://testemrs3/song_data/*/*/*/*.json'
logPath = 's3a://testemrs3/log_data/*.json'
</code></pre>
<pre><code class="language-python"># define output paths
output = 's3a://testemrs3/schema/'
</code></pre>
<h2 id="process-song-data">process song data</h2>
<h3 id="create-song_table">create song_table</h3>
<pre><code class="language-python"># Step 1: Read in the song data
df_song = spark.read.json(songPath)
</code></pre>
<pre><code class="language-python"># check the schema
df_song.printSchema()
</code></pre>
<pre><code>root
 |-- artist_id: string (nullable = true)
 |-- artist_latitude: double (nullable = true)
 |-- artist_location: string (nullable = true)
 |-- artist_longitude: double (nullable = true)
 |-- artist_name: string (nullable = true)
 |-- duration: double (nullable = true)
 |-- num_songs: long (nullable = true)
 |-- song_id: string (nullable = true)
 |-- title: string (nullable = true)
 |-- year: long (nullable = true)
</code></pre>
<pre><code class="language-python"># Step 2: extract columns to create songs table
song_cols = ['song_id', 'title', 'artist_id', 'year', 'duration']
</code></pre>
<pre><code class="language-python"># groupby song_id and select the first record's title in the group.
t1 = df_song.select(F.col('song_id'), 'title') \
    .groupBy('song_id') \
    .agg({'title': 'first'}) \
    .withColumnRenamed('first(title)', 'title1')

t2 = df_song.select(song_cols)
</code></pre>
<pre><code class="language-python">t1.toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>song_id</th>
      <th>title1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SOGOSOV12AF72A285E</td>
      <td>¿Dónde va Chichi?</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SOMZWCG12A8C13C480</td>
      <td>I Didn't Mean To</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SOUPIRU12A6D4FA1E1</td>
      <td>Der Kleine Dompfaff</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SOXVLOJ12AB0189215</td>
      <td>Amor De Cabaret</td>
    </tr>
    <tr>
      <th>4</th>
      <td>SOWTBJW12AC468AC6E</td>
      <td>Broken-Down Merry-Go-Round</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">t2.toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>song_id</th>
      <th>title</th>
      <th>artist_id</th>
      <th>year</th>
      <th>duration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SOBAYLL12A8C138AF9</td>
      <td>Sono andati? Fingevo di dormire</td>
      <td>ARDR4AC1187FB371A1</td>
      <td>0</td>
      <td>511.16363</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SOOLYAZ12A6701F4A6</td>
      <td>Laws Patrolling (Album Version)</td>
      <td>AREBBGV1187FB523D2</td>
      <td>0</td>
      <td>173.66159</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SOBBUGU12A8C13E95D</td>
      <td>Setting Fire to Sleeping Giants</td>
      <td>ARMAC4T1187FB3FA4C</td>
      <td>2004</td>
      <td>207.77751</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SOAOIBZ12AB01815BE</td>
      <td>I Hold Your Hand In Mine [Live At Royal Albert...</td>
      <td>ARPBNLO1187FB3D52F</td>
      <td>2000</td>
      <td>43.36281</td>
    </tr>
    <tr>
      <th>4</th>
      <td>SONYPOM12A8C13B2D7</td>
      <td>I Think My Wife Is Running Around On Me (Taco ...</td>
      <td>ARDNS031187B9924F0</td>
      <td>2005</td>
      <td>186.48771</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">song_table_df = t1.join(t2, 'song_id') \
                .where(F.col(&quot;title1&quot;) == F.col(&quot;title&quot;)) \
                .select(song_cols)
</code></pre>
<pre><code class="language-python">song_table_df.toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>song_id</th>
      <th>title</th>
      <th>artist_id</th>
      <th>year</th>
      <th>duration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SOBAYLL12A8C138AF9</td>
      <td>Sono andati? Fingevo di dormire</td>
      <td>ARDR4AC1187FB371A1</td>
      <td>0</td>
      <td>511.16363</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SOOLYAZ12A6701F4A6</td>
      <td>Laws Patrolling (Album Version)</td>
      <td>AREBBGV1187FB523D2</td>
      <td>0</td>
      <td>173.66159</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SOBBUGU12A8C13E95D</td>
      <td>Setting Fire to Sleeping Giants</td>
      <td>ARMAC4T1187FB3FA4C</td>
      <td>2004</td>
      <td>207.77751</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SOAOIBZ12AB01815BE</td>
      <td>I Hold Your Hand In Mine [Live At Royal Albert...</td>
      <td>ARPBNLO1187FB3D52F</td>
      <td>2000</td>
      <td>43.36281</td>
    </tr>
    <tr>
      <th>4</th>
      <td>SONYPOM12A8C13B2D7</td>
      <td>I Think My Wife Is Running Around On Me (Taco ...</td>
      <td>ARDNS031187B9924F0</td>
      <td>2005</td>
      <td>186.48771</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">song_table_df.toPandas().shape
</code></pre>
<pre><code>(71, 5)
</code></pre>
<pre><code class="language-python">df_song.toPandas().shape
</code></pre>
<pre><code>(71, 10)
</code></pre>
<pre><code class="language-python"># Step 3: Write this to a parquet file
song_table_df.write.parquet('data/songs_table', partitionBy=['year', 'artist_id'], mode='Overwrite')
</code></pre>
<pre><code class="language-python"># write this to s3 bucket
song_table_df.write.parquet(output + 'songs_table', partitionBy=['year', 'artist_id'], mode='Overwrite')
</code></pre>
<h3 id="create-artists_table">create artists_table</h3>
<pre><code class="language-python"># define the cols
artists_cols = [&quot;artist_id&quot;, &quot;artist_name&quot;, &quot;artist_location&quot;, &quot;artist_latitude&quot;, &quot;artist_longitude&quot;]
</code></pre>
<pre><code class="language-python">df_song.printSchema()
</code></pre>
<pre><code>root
 |-- artist_id: string (nullable = true)
 |-- artist_latitude: double (nullable = true)
 |-- artist_location: string (nullable = true)
 |-- artist_longitude: double (nullable = true)
 |-- artist_name: string (nullable = true)
 |-- duration: double (nullable = true)
 |-- num_songs: long (nullable = true)
 |-- song_id: string (nullable = true)
 |-- title: string (nullable = true)
 |-- year: long (nullable = true)
</code></pre>
<pre><code class="language-python"># groupby song_id and select the first record's title in the group.
t1 = df_song.select(F.col('artist_id'), 'artist_name') \
    .groupBy('artist_id') \
    .agg({'artist_name': 'first'}) \
    .withColumnRenamed('first(artist_name)', 'artist_name1')

t2 = df_song.select(artists_cols)
</code></pre>
<pre><code class="language-python">t1.toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>artist_id</th>
      <th>artist_name1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>AR9AWNF1187B9AB0B4</td>
      <td>Kenny G featuring Daryl Hall</td>
    </tr>
    <tr>
      <th>1</th>
      <td>AR0IAWL1187B9A96D0</td>
      <td>Danilo Perez</td>
    </tr>
    <tr>
      <th>2</th>
      <td>AR0RCMP1187FB3F427</td>
      <td>Billie Jo Spears</td>
    </tr>
    <tr>
      <th>3</th>
      <td>AREDL271187FB40F44</td>
      <td>Soul Mekanik</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ARI3BMM1187FB4255E</td>
      <td>Alice Stuart</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">t2.toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>artist_id</th>
      <th>artist_name</th>
      <th>artist_location</th>
      <th>artist_latitude</th>
      <th>artist_longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ARDR4AC1187FB371A1</td>
      <td>Montserrat Caballé;Placido Domingo;Vicente Sar...</td>
      <td></td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>AREBBGV1187FB523D2</td>
      <td>Mike Jones (Featuring CJ_ Mello &amp; Lil' Bran)</td>
      <td>Houston, TX</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ARMAC4T1187FB3FA4C</td>
      <td>The Dillinger Escape Plan</td>
      <td>Morris Plains, NJ</td>
      <td>40.82624</td>
      <td>-74.47995</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ARPBNLO1187FB3D52F</td>
      <td>Tiny Tim</td>
      <td>New York, NY</td>
      <td>40.71455</td>
      <td>-74.00712</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ARDNS031187B9924F0</td>
      <td>Tim Wilson</td>
      <td>Georgia</td>
      <td>32.67828</td>
      <td>-83.22295</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">artists_table_df = t1.join(t2, 'artist_id') \
                .where(F.col(&quot;artist_name1&quot;) == F.col(&quot;artist_name&quot;)) \
                .select(artists_cols)
</code></pre>
<pre><code class="language-python">artists_table_df.toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>artist_id</th>
      <th>artist_name</th>
      <th>artist_location</th>
      <th>artist_latitude</th>
      <th>artist_longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ARDR4AC1187FB371A1</td>
      <td>Montserrat Caballé;Placido Domingo;Vicente Sar...</td>
      <td></td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>AREBBGV1187FB523D2</td>
      <td>Mike Jones (Featuring CJ_ Mello &amp; Lil' Bran)</td>
      <td>Houston, TX</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ARMAC4T1187FB3FA4C</td>
      <td>The Dillinger Escape Plan</td>
      <td>Morris Plains, NJ</td>
      <td>40.82624</td>
      <td>-74.47995</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ARPBNLO1187FB3D52F</td>
      <td>Tiny Tim</td>
      <td>New York, NY</td>
      <td>40.71455</td>
      <td>-74.00712</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ARDNS031187B9924F0</td>
      <td>Tim Wilson</td>
      <td>Georgia</td>
      <td>32.67828</td>
      <td>-83.22295</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python"># write this to s3 bucket
artists_table_df.write.parquet(output + 'artists_table', mode='Overwrite')
</code></pre>
<pre><code class="language-python">artists_table_df.write.parquet('data/artists_table', mode='Overwrite')
</code></pre>
<pre><code class="language-python"># read the partitioned data
df_artists_read = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;data/artists_table&quot;)
</code></pre>
<h2 id="process-log-data">Process log data</h2>
<pre><code class="language-python"># Step 1: Read in the log data
df_log = spark.read.json(logPath)
</code></pre>
<pre><code class="language-python">df_log.printSchema()
</code></pre>
<pre><code>root
 |-- artist: string (nullable = true)
 |-- auth: string (nullable = true)
 |-- firstName: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- itemInSession: long (nullable = true)
 |-- lastName: string (nullable = true)
 |-- length: double (nullable = true)
 |-- level: string (nullable = true)
 |-- location: string (nullable = true)
 |-- method: string (nullable = true)
 |-- page: string (nullable = true)
 |-- registration: double (nullable = true)
 |-- sessionId: long (nullable = true)
 |-- song: string (nullable = true)
 |-- status: long (nullable = true)
 |-- ts: long (nullable = true)
 |-- userAgent: string (nullable = true)
 |-- userId: string (nullable = true)
</code></pre>
<pre><code class="language-python">df_log.filter(F.col(&quot;page&quot;) == &quot;NextSong&quot;).toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>artist</th>
      <th>auth</th>
      <th>firstName</th>
      <th>gender</th>
      <th>itemInSession</th>
      <th>lastName</th>
      <th>length</th>
      <th>level</th>
      <th>location</th>
      <th>method</th>
      <th>page</th>
      <th>registration</th>
      <th>sessionId</th>
      <th>song</th>
      <th>status</th>
      <th>ts</th>
      <th>userAgent</th>
      <th>userId</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Harmonia</td>
      <td>Logged In</td>
      <td>Ryan</td>
      <td>M</td>
      <td>0</td>
      <td>Smith</td>
      <td>655.77751</td>
      <td>free</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>PUT</td>
      <td>NextSong</td>
      <td>1.541017e+12</td>
      <td>583</td>
      <td>Sehr kosmisch</td>
      <td>200</td>
      <td>1542241826796</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>26</td>
    </tr>
    <tr>
      <th>1</th>
      <td>The Prodigy</td>
      <td>Logged In</td>
      <td>Ryan</td>
      <td>M</td>
      <td>1</td>
      <td>Smith</td>
      <td>260.07465</td>
      <td>free</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>PUT</td>
      <td>NextSong</td>
      <td>1.541017e+12</td>
      <td>583</td>
      <td>The Big Gundown</td>
      <td>200</td>
      <td>1542242481796</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>26</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Train</td>
      <td>Logged In</td>
      <td>Ryan</td>
      <td>M</td>
      <td>2</td>
      <td>Smith</td>
      <td>205.45261</td>
      <td>free</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>PUT</td>
      <td>NextSong</td>
      <td>1.541017e+12</td>
      <td>583</td>
      <td>Marry Me</td>
      <td>200</td>
      <td>1542242741796</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>26</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Sony Wonder</td>
      <td>Logged In</td>
      <td>Samuel</td>
      <td>M</td>
      <td>0</td>
      <td>Gonzalez</td>
      <td>218.06975</td>
      <td>free</td>
      <td>Houston-The Woodlands-Sugar Land, TX</td>
      <td>PUT</td>
      <td>NextSong</td>
      <td>1.540493e+12</td>
      <td>597</td>
      <td>Blackbird</td>
      <td>200</td>
      <td>1542253449796</td>
      <td>"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>
      <td>61</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Van Halen</td>
      <td>Logged In</td>
      <td>Tegan</td>
      <td>F</td>
      <td>2</td>
      <td>Levine</td>
      <td>289.38404</td>
      <td>paid</td>
      <td>Portland-South Portland, ME</td>
      <td>PUT</td>
      <td>NextSong</td>
      <td>1.540794e+12</td>
      <td>602</td>
      <td>Best Of Both Worlds (Remastered Album Version)</td>
      <td>200</td>
      <td>1542260935796</td>
      <td>"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>
      <td>80</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python"># Step 2: filter by actions for song plays
df_log = df_log.filter(F.col(&quot;page&quot;) == &quot;NextSong&quot;)
</code></pre>
<pre><code class="language-python">df_log.toPandas().shape
</code></pre>
<pre><code>(6820, 18)
</code></pre>
<pre><code class="language-python"># Step 3: extract columns for users table
users_cols = [&quot;userId&quot;, &quot;firstName&quot;, &quot;lastName&quot;, &quot;gender&quot;, &quot;level&quot;]
</code></pre>
<pre><code class="language-python">df_log.select(users_cols).toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>userId</th>
      <th>firstName</th>
      <th>lastName</th>
      <th>gender</th>
      <th>level</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>26</td>
      <td>Ryan</td>
      <td>Smith</td>
      <td>M</td>
      <td>free</td>
    </tr>
    <tr>
      <th>1</th>
      <td>26</td>
      <td>Ryan</td>
      <td>Smith</td>
      <td>M</td>
      <td>free</td>
    </tr>
    <tr>
      <th>2</th>
      <td>26</td>
      <td>Ryan</td>
      <td>Smith</td>
      <td>M</td>
      <td>free</td>
    </tr>
    <tr>
      <th>3</th>
      <td>61</td>
      <td>Samuel</td>
      <td>Gonzalez</td>
      <td>M</td>
      <td>free</td>
    </tr>
    <tr>
      <th>4</th>
      <td>80</td>
      <td>Tegan</td>
      <td>Levine</td>
      <td>F</td>
      <td>paid</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">df_log.select(users_cols).toPandas().shape
</code></pre>
<pre><code>(6820, 5)
</code></pre>
<pre><code class="language-python">df_log.select(users_cols).dropDuplicates().toPandas().shape
</code></pre>
<pre><code>(104, 5)
</code></pre>
<pre><code class="language-python">df_log.select(users_cols).dropDuplicates().toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>userId</th>
      <th>firstName</th>
      <th>lastName</th>
      <th>gender</th>
      <th>level</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>57</td>
      <td>Katherine</td>
      <td>Gay</td>
      <td>F</td>
      <td>free</td>
    </tr>
    <tr>
      <th>1</th>
      <td>84</td>
      <td>Shakira</td>
      <td>Hunt</td>
      <td>F</td>
      <td>free</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>Sean</td>
      <td>Wilson</td>
      <td>F</td>
      <td>free</td>
    </tr>
    <tr>
      <th>3</th>
      <td>52</td>
      <td>Theodore</td>
      <td>Smith</td>
      <td>M</td>
      <td>free</td>
    </tr>
    <tr>
      <th>4</th>
      <td>80</td>
      <td>Tegan</td>
      <td>Levine</td>
      <td>F</td>
      <td>paid</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">users_table_df = df_log.select(users_cols).dropDuplicates()
</code></pre>
<pre><code class="language-python"># write this to s3 bucket
users_table_df.write.parquet(output + 'users_table', mode='Overwrite')
</code></pre>
<pre><code class="language-python">users_table_df.write.parquet('data/users_table', mode='Overwrite')
</code></pre>
<h2 id="time-table">Time table</h2>
<pre><code class="language-python"># # create timestamp column from original timestamp column
get_timestamp = udf()
</code></pre>
<pre><code class="language-python">df_log.select('ts').toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1542241826796</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1542242481796</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1542242741796</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1542253449796</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1542260935796</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">df.withColumn('epoch', f.date_format(df.epoch.cast(dataType=t.TimestampType()), &quot;yyyy-MM-dd&quot;))
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">df_log.withColumn('ts', F.date_format(df_log.ts.cast(dataType=T.TimestampType()), &quot;yyyy-MM-dd&quot;)).toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>artist</th>
      <th>auth</th>
      <th>firstName</th>
      <th>gender</th>
      <th>itemInSession</th>
      <th>lastName</th>
      <th>length</th>
      <th>level</th>
      <th>location</th>
      <th>method</th>
      <th>page</th>
      <th>registration</th>
      <th>sessionId</th>
      <th>song</th>
      <th>status</th>
      <th>ts</th>
      <th>userAgent</th>
      <th>userId</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Harmonia</td>
      <td>Logged In</td>
      <td>Ryan</td>
      <td>M</td>
      <td>0</td>
      <td>Smith</td>
      <td>655.77751</td>
      <td>free</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>PUT</td>
      <td>NextSong</td>
      <td>1.541017e+12</td>
      <td>583</td>
      <td>Sehr kosmisch</td>
      <td>200</td>
      <td>50841-09-12</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>26</td>
    </tr>
    <tr>
      <th>1</th>
      <td>The Prodigy</td>
      <td>Logged In</td>
      <td>Ryan</td>
      <td>M</td>
      <td>1</td>
      <td>Smith</td>
      <td>260.07465</td>
      <td>free</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>PUT</td>
      <td>NextSong</td>
      <td>1.541017e+12</td>
      <td>583</td>
      <td>The Big Gundown</td>
      <td>200</td>
      <td>50841-09-19</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>26</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Train</td>
      <td>Logged In</td>
      <td>Ryan</td>
      <td>M</td>
      <td>2</td>
      <td>Smith</td>
      <td>205.45261</td>
      <td>free</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>PUT</td>
      <td>NextSong</td>
      <td>1.541017e+12</td>
      <td>583</td>
      <td>Marry Me</td>
      <td>200</td>
      <td>50841-09-22</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>26</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Sony Wonder</td>
      <td>Logged In</td>
      <td>Samuel</td>
      <td>M</td>
      <td>0</td>
      <td>Gonzalez</td>
      <td>218.06975</td>
      <td>free</td>
      <td>Houston-The Woodlands-Sugar Land, TX</td>
      <td>PUT</td>
      <td>NextSong</td>
      <td>1.540493e+12</td>
      <td>597</td>
      <td>Blackbird</td>
      <td>200</td>
      <td>50842-01-24</td>
      <td>"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>
      <td>61</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Van Halen</td>
      <td>Logged In</td>
      <td>Tegan</td>
      <td>F</td>
      <td>2</td>
      <td>Levine</td>
      <td>289.38404</td>
      <td>paid</td>
      <td>Portland-South Portland, ME</td>
      <td>PUT</td>
      <td>NextSong</td>
      <td>1.540794e+12</td>
      <td>602</td>
      <td>Best Of Both Worlds (Remastered Album Version)</td>
      <td>200</td>
      <td>50842-04-21</td>
      <td>"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>
      <td>80</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">df_time = df_log.select('ts')
</code></pre>
<pre><code class="language-python">df_time.take(5)
</code></pre>
<pre><code>[Row(ts=1542241826796),
 Row(ts=1542242481796),
 Row(ts=1542242741796),
 Row(ts=1542253449796),
 Row(ts=1542260935796)]
</code></pre>
<pre><code class="language-python">@udf
def gettimestamp(time):
    import datetime
    time = time/1000
    return datetime.datetime.fromtimestamp(time).strftime(&quot;%m-%d-%Y %H:%M:%S&quot;)
</code></pre>
<pre><code class="language-python">df_time.withColumn(&quot;timestamp&quot;, gettimestamp(&quot;ts&quot;)).show()
</code></pre>
<pre><code>+-------------+-------------------+
|           ts|          timestamp|
+-------------+-------------------+
|1542241826796|11-15-2018 00:30:26|
|1542242481796|11-15-2018 00:41:21|
|1542242741796|11-15-2018 00:45:41|
|1542253449796|11-15-2018 03:44:09|
|1542260935796|11-15-2018 05:48:55|
|1542261224796|11-15-2018 05:53:44|
|1542261356796|11-15-2018 05:55:56|
|1542261662796|11-15-2018 06:01:02|
|1542262057796|11-15-2018 06:07:37|
|1542262233796|11-15-2018 06:10:33|
|1542262434796|11-15-2018 06:13:54|
|1542262456796|11-15-2018 06:14:16|
|1542262679796|11-15-2018 06:17:59|
|1542262728796|11-15-2018 06:18:48|
|1542262893796|11-15-2018 06:21:33|
|1542263158796|11-15-2018 06:25:58|
|1542263378796|11-15-2018 06:29:38|
|1542265716796|11-15-2018 07:08:36|
|1542265929796|11-15-2018 07:12:09|
|1542266927796|11-15-2018 07:28:47|
+-------------+-------------------+
only showing top 20 rows
</code></pre>
<pre><code class="language-python">df_time.printSchema()
</code></pre>
<pre><code>root
 |-- ts: long (nullable = true)
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">get_timestamp = F.udf(lambda x: datetime.fromtimestamp( (x/1000.0) ), T.TimestampType()) 
get_hour = F.udf(lambda x: x.hour, T.IntegerType()) 
get_day = F.udf(lambda x: x.day, T.IntegerType()) 
get_week = F.udf(lambda x: x.isocalendar()[1], T.IntegerType()) 
get_month = F.udf(lambda x: x.month, T.IntegerType()) 
get_year = F.udf(lambda x: x.year, T.IntegerType()) 
get_weekday = F.udf(lambda x: x.weekday(), T.IntegerType()) 
</code></pre>
<pre><code class="language-python">df_log = df_log.withColumn(&quot;timestamp&quot;, get_timestamp(df_log.ts))
df_log = df_log.withColumn(&quot;hour&quot;, get_hour(df_log.timestamp))
df_log = df_log.withColumn(&quot;day&quot;, get_day(df_log.timestamp))
df_log = df_log.withColumn(&quot;week&quot;, get_week(df_log.timestamp))
df_log = df_log.withColumn(&quot;month&quot;, get_month(df_log.timestamp))
df_log = df_log.withColumn(&quot;year&quot;, get_year(df_log.timestamp))
df_log = df_log.withColumn(&quot;weekday&quot;, get_weekday(df_log.timestamp))
df_log.limit(5).toPandas()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>artist</th>
      <th>auth</th>
      <th>firstName</th>
      <th>gender</th>
      <th>itemInSession</th>
      <th>lastName</th>
      <th>length</th>
      <th>level</th>
      <th>location</th>
      <th>method</th>
      <th>...</th>
      <th>ts</th>
      <th>userAgent</th>
      <th>userId</th>
      <th>timestamp</th>
      <th>hour</th>
      <th>day</th>
      <th>week</th>
      <th>month</th>
      <th>year</th>
      <th>weekday</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Harmonia</td>
      <td>Logged In</td>
      <td>Ryan</td>
      <td>M</td>
      <td>0</td>
      <td>Smith</td>
      <td>655.77751</td>
      <td>free</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>PUT</td>
      <td>...</td>
      <td>1542241826796</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>26</td>
      <td>2018-11-15 00:30:26.796</td>
      <td>0</td>
      <td>15</td>
      <td>46</td>
      <td>11</td>
      <td>2018</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>The Prodigy</td>
      <td>Logged In</td>
      <td>Ryan</td>
      <td>M</td>
      <td>1</td>
      <td>Smith</td>
      <td>260.07465</td>
      <td>free</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>PUT</td>
      <td>...</td>
      <td>1542242481796</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>26</td>
      <td>2018-11-15 00:41:21.796</td>
      <td>0</td>
      <td>15</td>
      <td>46</td>
      <td>11</td>
      <td>2018</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Train</td>
      <td>Logged In</td>
      <td>Ryan</td>
      <td>M</td>
      <td>2</td>
      <td>Smith</td>
      <td>205.45261</td>
      <td>free</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>PUT</td>
      <td>...</td>
      <td>1542242741796</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>26</td>
      <td>2018-11-15 00:45:41.796</td>
      <td>0</td>
      <td>15</td>
      <td>46</td>
      <td>11</td>
      <td>2018</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Sony Wonder</td>
      <td>Logged In</td>
      <td>Samuel</td>
      <td>M</td>
      <td>0</td>
      <td>Gonzalez</td>
      <td>218.06975</td>
      <td>free</td>
      <td>Houston-The Woodlands-Sugar Land, TX</td>
      <td>PUT</td>
      <td>...</td>
      <td>1542253449796</td>
      <td>"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>
      <td>61</td>
      <td>2018-11-15 03:44:09.796</td>
      <td>3</td>
      <td>15</td>
      <td>46</td>
      <td>11</td>
      <td>2018</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Van Halen</td>
      <td>Logged In</td>
      <td>Tegan</td>
      <td>F</td>
      <td>2</td>
      <td>Levine</td>
      <td>289.38404</td>
      <td>paid</td>
      <td>Portland-South Portland, ME</td>
      <td>PUT</td>
      <td>...</td>
      <td>1542260935796</td>
      <td>"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>
      <td>80</td>
      <td>2018-11-15 05:48:55.796</td>
      <td>5</td>
      <td>15</td>
      <td>46</td>
      <td>11</td>
      <td>2018</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 25 columns</p>
</div>
<pre><code class="language-python">time_cols = [&quot;timestamp&quot;, &quot;hour&quot;, &quot;day&quot;, &quot;week&quot;, &quot;month&quot;, &quot;year&quot;, &quot;weekday&quot;]
</code></pre>
<pre><code class="language-python">time_table_df = df_log.select(time_cols)
</code></pre>
<pre><code class="language-python"># write to parquet file partition by 
time_table_df.write.parquet('data/time_table', partitionBy=['year', 'month'], mode='Overwrite')
</code></pre>
<pre><code class="language-python"># write this to s3 bucket
time_table_df.write.parquet(output + 'time_table', partitionBy=['year', 'month'], mode='Overwrite')
</code></pre>
<h2 id="songplay-table">SongPlay table</h2>
<pre><code class="language-python">df_log.printSchema()
</code></pre>
<pre><code>root
 |-- artist: string (nullable = true)
 |-- auth: string (nullable = true)
 |-- firstName: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- itemInSession: long (nullable = true)
 |-- lastName: string (nullable = true)
 |-- length: double (nullable = true)
 |-- level: string (nullable = true)
 |-- location: string (nullable = true)
 |-- method: string (nullable = true)
 |-- page: string (nullable = true)
 |-- registration: double (nullable = true)
 |-- sessionId: long (nullable = true)
 |-- song: string (nullable = true)
 |-- status: long (nullable = true)
 |-- ts: long (nullable = true)
 |-- userAgent: string (nullable = true)
 |-- userId: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- hour: integer (nullable = true)
 |-- day: integer (nullable = true)
 |-- week: integer (nullable = true)
 |-- month: integer (nullable = true)
 |-- year: integer (nullable = true)
 |-- weekday: integer (nullable = true)
</code></pre>
<pre><code class="language-python">songplay_cols_temp = [&quot;timestamp&quot;, &quot;userId&quot;, &quot;sessionId&quot;, &quot;location&quot;, &quot;userAgent&quot;, &quot;level&quot;]
</code></pre>
<pre><code class="language-python">df_log.select(songplay_cols).toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>timestamp</th>
      <th>userId</th>
      <th>sessionId</th>
      <th>location</th>
      <th>userAgent</th>
      <th>level</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2018-11-15 00:30:26.796</td>
      <td>26</td>
      <td>583</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>free</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2018-11-15 00:41:21.796</td>
      <td>26</td>
      <td>583</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>free</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2018-11-15 00:45:41.796</td>
      <td>26</td>
      <td>583</td>
      <td>San Jose-Sunnyvale-Santa Clara, CA</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>free</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2018-11-15 03:44:09.796</td>
      <td>61</td>
      <td>597</td>
      <td>Houston-The Woodlands-Sugar Land, TX</td>
      <td>"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>
      <td>free</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2018-11-15 05:48:55.796</td>
      <td>80</td>
      <td>602</td>
      <td>Portland-South Portland, ME</td>
      <td>"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>
      <td>paid</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python"># read the partitioned data
df_artists_read = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;data/artists_table&quot;)
df_songs_read = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;data/songs_table&quot;)
</code></pre>
<pre><code class="language-python">df_artists_read.toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>artist_id</th>
      <th>artist_name</th>
      <th>artist_location</th>
      <th>artist_latitude</th>
      <th>artist_longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ARDR4AC1187FB371A1</td>
      <td>Montserrat Caballé;Placido Domingo;Vicente Sar...</td>
      <td></td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>AREBBGV1187FB523D2</td>
      <td>Mike Jones (Featuring CJ_ Mello &amp; Lil' Bran)</td>
      <td>Houston, TX</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ARMAC4T1187FB3FA4C</td>
      <td>The Dillinger Escape Plan</td>
      <td>Morris Plains, NJ</td>
      <td>40.82624</td>
      <td>-74.47995</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ARPBNLO1187FB3D52F</td>
      <td>Tiny Tim</td>
      <td>New York, NY</td>
      <td>40.71455</td>
      <td>-74.00712</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ARDNS031187B9924F0</td>
      <td>Tim Wilson</td>
      <td>Georgia</td>
      <td>32.67828</td>
      <td>-83.22295</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">df_songs_read.toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>song_id</th>
      <th>title</th>
      <th>duration</th>
      <th>year</th>
      <th>artist_id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SOAOIBZ12AB01815BE</td>
      <td>I Hold Your Hand In Mine [Live At Royal Albert...</td>
      <td>43.36281</td>
      <td>2000</td>
      <td>ARPBNLO1187FB3D52F</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SONYPOM12A8C13B2D7</td>
      <td>I Think My Wife Is Running Around On Me (Taco ...</td>
      <td>186.48771</td>
      <td>2005</td>
      <td>ARDNS031187B9924F0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SODREIN12A58A7F2E5</td>
      <td>A Whiter Shade Of Pale (Live @ Fillmore West)</td>
      <td>326.00771</td>
      <td>0</td>
      <td>ARLTWXK1187FB5A3F8</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SOYMRWW12A6D4FAB14</td>
      <td>The Moon And I (Ordinary Day Album Version)</td>
      <td>267.70240</td>
      <td>0</td>
      <td>ARKFYS91187B98E58F</td>
    </tr>
    <tr>
      <th>4</th>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>Streets On Fire (Explicit Album Version)</td>
      <td>279.97995</td>
      <td>0</td>
      <td>ARPFHN61187FB575F6</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python"># merge song and artists
df_songs_read.join(df_artists_read, 'artist_id').toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>artist_id</th>
      <th>song_id</th>
      <th>title</th>
      <th>duration</th>
      <th>year</th>
      <th>artist_name</th>
      <th>artist_location</th>
      <th>artist_latitude</th>
      <th>artist_longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ARPBNLO1187FB3D52F</td>
      <td>SOAOIBZ12AB01815BE</td>
      <td>I Hold Your Hand In Mine [Live At Royal Albert...</td>
      <td>43.36281</td>
      <td>2000</td>
      <td>Tiny Tim</td>
      <td>New York, NY</td>
      <td>40.71455</td>
      <td>-74.00712</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ARDNS031187B9924F0</td>
      <td>SONYPOM12A8C13B2D7</td>
      <td>I Think My Wife Is Running Around On Me (Taco ...</td>
      <td>186.48771</td>
      <td>2005</td>
      <td>Tim Wilson</td>
      <td>Georgia</td>
      <td>32.67828</td>
      <td>-83.22295</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ARLTWXK1187FB5A3F8</td>
      <td>SODREIN12A58A7F2E5</td>
      <td>A Whiter Shade Of Pale (Live @ Fillmore West)</td>
      <td>326.00771</td>
      <td>0</td>
      <td>King Curtis</td>
      <td>Fort Worth, TX</td>
      <td>32.74863</td>
      <td>-97.32925</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ARKFYS91187B98E58F</td>
      <td>SOYMRWW12A6D4FAB14</td>
      <td>The Moon And I (Ordinary Day Album Version)</td>
      <td>267.70240</td>
      <td>0</td>
      <td>Jeff And Sheri Easter</td>
      <td></td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ARPFHN61187FB575F6</td>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>Streets On Fire (Explicit Album Version)</td>
      <td>279.97995</td>
      <td>0</td>
      <td>Lupe Fiasco</td>
      <td>Chicago, IL</td>
      <td>41.88415</td>
      <td>-87.63241</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">df_joined_songs_artists = df_songs_read.join(df_artists_read, 'artist_id').select(&quot;artist_id&quot;, &quot;song_id&quot;, &quot;title&quot;, &quot;artist_name&quot;)
</code></pre>
<pre><code class="language-python">songplay_cols = [&quot;timestamp&quot;, &quot;userId&quot;, &quot;song_id&quot;, &quot;artist_id&quot;, &quot;sessionId&quot;, &quot;location&quot;, &quot;userAgent&quot;, &quot;level&quot;, &quot;month&quot;, &quot;year&quot;]
</code></pre>
<pre><code class="language-python"># join df_logs with df_joined_songs_artists
df_log.join(df_joined_songs_artists, df_log.artist == df_joined_songs_artists.artist_name).select(songplay_cols).toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>timestamp</th>
      <th>userId</th>
      <th>song_id</th>
      <th>artist_id</th>
      <th>sessionId</th>
      <th>location</th>
      <th>userAgent</th>
      <th>level</th>
      <th>month</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2018-11-10 07:47:51.796</td>
      <td>44</td>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>ARPFHN61187FB575F6</td>
      <td>350</td>
      <td>Waterloo-Cedar Falls, IA</td>
      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; r...</td>
      <td>paid</td>
      <td>11</td>
      <td>2018</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2018-11-06 18:34:31.796</td>
      <td>97</td>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>ARPFHN61187FB575F6</td>
      <td>293</td>
      <td>Lansing-East Lansing, MI</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>paid</td>
      <td>11</td>
      <td>2018</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2018-11-06 16:04:44.796</td>
      <td>2</td>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>ARPFHN61187FB575F6</td>
      <td>126</td>
      <td>Plymouth, IN</td>
      <td>"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>
      <td>free</td>
      <td>11</td>
      <td>2018</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2018-11-28 23:22:57.796</td>
      <td>24</td>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>ARPFHN61187FB575F6</td>
      <td>984</td>
      <td>Lake Havasu City-Kingman, AZ</td>
      <td>"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>
      <td>paid</td>
      <td>11</td>
      <td>2018</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2018-11-14 13:11:26.796</td>
      <td>34</td>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>ARPFHN61187FB575F6</td>
      <td>495</td>
      <td>Milwaukee-Waukesha-West Allis, WI</td>
      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; r...</td>
      <td>free</td>
      <td>11</td>
      <td>2018</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">songplay_table_df = df_log.join(df_joined_songs_artists, df_log.artist == df_joined_songs_artists.artist_name).select(songplay_cols)
songplay_table_df = songplay_table_df.withColumn(&quot;songplay_id&quot;, F.monotonically_increasing_id())
</code></pre>
<pre><code class="language-python">songplay_table_df.toPandas().head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>timestamp</th>
      <th>userId</th>
      <th>song_id</th>
      <th>artist_id</th>
      <th>sessionId</th>
      <th>location</th>
      <th>userAgent</th>
      <th>level</th>
      <th>month</th>
      <th>year</th>
      <th>songplay_id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2018-11-10 07:47:51.796</td>
      <td>44</td>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>ARPFHN61187FB575F6</td>
      <td>350</td>
      <td>Waterloo-Cedar Falls, IA</td>
      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; r...</td>
      <td>paid</td>
      <td>11</td>
      <td>2018</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2018-11-06 18:34:31.796</td>
      <td>97</td>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>ARPFHN61187FB575F6</td>
      <td>293</td>
      <td>Lansing-East Lansing, MI</td>
      <td>"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>
      <td>paid</td>
      <td>11</td>
      <td>2018</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2018-11-06 16:04:44.796</td>
      <td>2</td>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>ARPFHN61187FB575F6</td>
      <td>126</td>
      <td>Plymouth, IN</td>
      <td>"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>
      <td>free</td>
      <td>11</td>
      <td>2018</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2018-11-28 23:22:57.796</td>
      <td>24</td>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>ARPFHN61187FB575F6</td>
      <td>984</td>
      <td>Lake Havasu City-Kingman, AZ</td>
      <td>"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>
      <td>paid</td>
      <td>11</td>
      <td>2018</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2018-11-14 13:11:26.796</td>
      <td>34</td>
      <td>SOWQTQZ12A58A7B63E</td>
      <td>ARPFHN61187FB575F6</td>
      <td>495</td>
      <td>Milwaukee-Waukesha-West Allis, WI</td>
      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; r...</td>
      <td>free</td>
      <td>11</td>
      <td>2018</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python"># write this to parquet file
# write to parquet file partition by 
songplay_table_df.write.parquet('data/songplays_table', partitionBy=['year', 'month'], mode='Overwrite')
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">from pyspark.sql import functions as F
</code></pre>
<pre><code class="language-python">from glob import glob
</code></pre>
<pre><code class="language-python">test_df = spark.read.json(glob(&quot;test/*.json&quot;))
</code></pre>
<pre><code class="language-python">test_df.select(['song_id', 'title', 'artist_id', 'year', 'duration']).toPandas()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>song_id</th>
      <th>title</th>
      <th>artist_id</th>
      <th>year</th>
      <th>duration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SOYMRWW12A6D4FAB14</td>
      <td>The Moon And I (Ordinary Day Album Version)</td>
      <td>ARKFYS91187B98E58F</td>
      <td>0</td>
      <td>267.70240</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SOHKNRJ12A6701D1F8</td>
      <td>Drop of Rain</td>
      <td>AR10USD1187B99F3F1</td>
      <td>0</td>
      <td>189.57016</td>
    </tr>
    <tr>
      <th>2</th>
      <td>SOYMRWW12A6D4FAB14</td>
      <td>The Moon And SUN</td>
      <td>ASKFYS91187B98E58F</td>
      <td>0</td>
      <td>269.70240</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SOUDSGM12AC9618304</td>
      <td>Insatiable (Instrumental Version)</td>
      <td>ARNTLGG11E2835DDB9</td>
      <td>0</td>
      <td>266.39628</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">test_df.select(['song_id', 'title', 'artist_id', 'year', 'duration']).groupBy('song_id').count().show()
</code></pre>
<pre><code>+------------------+-----+
|           song_id|count|
+------------------+-----+
|SOUDSGM12AC9618304|    1|
|SOYMRWW12A6D4FAB14|    2|
|SOHKNRJ12A6701D1F8|    1|
+------------------+-----+
</code></pre>
<pre><code class="language-python">test_df.select(F.col('song_id'), 'title') \
    .groupBy('song_id') \
    .agg({'title': 'first'}) \
    .withColumnRenamed('first(title)', 'title1') \
    .show()
</code></pre>
<pre><code>+------------------+--------------------+
|           song_id|               title|
+------------------+--------------------+
|SOUDSGM12AC9618304|Insatiable (Instr...|
|SOYMRWW12A6D4FAB14|The Moon And I (O...|
|SOHKNRJ12A6701D1F8|        Drop of Rain|
+------------------+--------------------+
</code></pre>
<pre><code class="language-python">t1 = test_df.select(F.col('song_id'), 'title') \
    .groupBy('song_id') \
    .agg({'title': 'first'}) \
    .withColumnRenamed('first(title)', 'title1')
</code></pre>
<pre><code class="language-python">t2 = test_df.select(['song_id', 'title', 'artist_id', 'year', 'duration'])
</code></pre>
<pre><code class="language-python">t1.join(t2, 'song_id').where(F.col(&quot;title1&quot;) == F.col(&quot;title&quot;)).select([&quot;song_id&quot;, &quot;title&quot;, &quot;artist_id&quot;, &quot;year&quot;, &quot;duration&quot;]).show()
</code></pre>
<pre><code>+------------------+--------------------+------------------+----+---------+
|           song_id|               title|         artist_id|year| duration|
+------------------+--------------------+------------------+----+---------+
|SOYMRWW12A6D4FAB14|The Moon And I (O...|ARKFYS91187B98E58F|   0| 267.7024|
|SOHKNRJ12A6701D1F8|        Drop of Rain|AR10USD1187B99F3F1|   0|189.57016|
|SOUDSGM12AC9618304|Insatiable (Instr...|ARNTLGG11E2835DDB9|   0|266.39628|
+------------------+--------------------+------------------+----+---------+
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Examples of System Design]]></title>
        <id>https://ywang412.github.io/post/system-design</id>
        <link href="https://ywang412.github.io/post/system-design">
        </link>
        <updated>2019-07-31T14:48:51.000Z</updated>
        <content type="html"><![CDATA[<figure data-type="image" tabindex="1"><img src="https://ywang412.github.io/post-images/1576380638596.png" alt=""></figure>
<p><img src="https://ywang412.github.io/post-images/1576385934910.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1576385942324.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1576385947273.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1576385965838.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1576386036018.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1576385974530.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1576385981729.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1576385988738.png" alt=""></p>
<p>byte 1 byte -128 to 127.<br>
short 2 bytes -32,768 to 32,767.<br>
int 4 bytes -2,147,483,648 to 2,147,483,647.<br>
long 8 bytes -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807.</p>
<p>Requirement clarification (use cases, post, follow, search, push notification)<br>
API<br>
Estimation (scaling, partitioning, load balancing, caching, storage, network bandwidth)<br>
Data model (SQL schema)</p>
<p><strong>Designing TinyURL</strong></p>
<p>Encoding actual URL<br>
We can compute a unique hash (e.g., MD5 or SHA256, etc.) of the given URL. The hash can then be encoded for displaying. This encoding could be base36 ([a-z ,0-9]) or base62 ([A-Z, a-z, 0-9]) and if we add ‘-’ and ‘.’ we can use base64 encoding. A reasonable question would be, what should be the length of the short key? 6, 8 or 10 characters.</p>
<p>Using base64 encoding, a 6 letter long key would result in 64^6 = ~68.7 billion possible strings<br>
Using base64 encoding, an 8 letter long key would result in 64^8 = ~281 trillion possible strings</p>
<p>With 68.7B unique strings, let’s assume six letter keys would suffice for our system.</p>
<p>If we use the MD5 algorithm as our hash function, it’ll produce a 128-bit hash value. After base64 encoding, we’ll get a string having more than 21 characters (since each base64 character encodes 6 bits of the hash value). Since we only have space for 8 characters per short key, how will we choose our key then? We can take the first 6 (or 8) letters for the key. This could result in key duplication though, upon which we can choose some other characters out of the encoding string or swap some characters.</p>
<p>What are different issues with our solution? We have the following couple of problems with our encoding scheme:</p>
<p>If multiple users enter the same URL, they can get the same shortened URL, which is not acceptable.<br>
What if parts of the URL are URL-encoded? e.g., http://www.educative.io/distributed.php?id=design, and http://www.educative.io/distributed.php%3Fid%3Ddesign are identical except for the URL encoding.<br>
Workaround for the issues: We can append an increasing sequence number to each input URL to make it unique, and then generate a hash of it. We don’t need to store this sequence number in the databases, though. Possible problems with this approach could be an ever-increasing sequence number. Can it overflow? Appending an increasing sequence number will also impact the performance of the service.</p>
<p>Another solution could be to append user id (which should be unique) to the input URL. However, if the user has not signed in, we would have to ask the user to choose a uniqueness key. Even after this, if we have a conflict, we have to keep generating a key until we get a unique one.</p>
<p><img src="https://ywang412.github.io/post-images/1565756686530.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1565754688299.png" alt=""></p>
<p><strong>Designing Pastebin</strong></p>
<p><img src="https://ywang412.github.io/post-images/1565757376688.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1565756747081.png" alt=""></p>
<p><strong>Design Instagram</strong></p>
<p><img src="https://ywang412.github.io/post-images/1565757489241.png" alt=""><br>
a. Partitioning based on UserID Let’s assume we shard based on the ‘UserID’ so that we can keep all photos of a user on the same shard. If one DB shard is 1TB, we will need four shards to store 3.7TB of data. Let’s assume for better performance and scalability we keep 10 shards.</p>
<p>So we’ll find the shard number by UserID % 10 and then store the data there. To uniquely identify any photo in our system, we can append shard number with each PhotoID.</p>
<p>How can we generate PhotoIDs? Each DB shard can have its own auto-increment sequence for PhotoIDs and since we will append ShardID with each PhotoID, it will make it unique throughout our system.</p>
<p>What are the different issues with this partitioning scheme?</p>
<p>How would we handle hot users? Several people follow such hot users and a lot of other people see any photo they upload.<br>
Some users will have a lot of photos compared to others, thus making a non-uniform distribution of storage.<br>
What if we cannot store all pictures of a user on one shard? If we distribute photos of a user onto multiple shards will it cause higher latencies?<br>
Storing all photos of a user on one shard can cause issues like unavailability of all of the user’s data if that shard is down or higher latency if it is serving high load etc.<br>
b. Partitioning based on PhotoID If we can generate unique PhotoIDs first and then find a shard number through “PhotoID % 10”, the above problems will have been solved. We would not need to append ShardID with PhotoID in this case as PhotoID will itself be unique throughout the system.</p>
<p>How can we generate PhotoIDs? Here we cannot have an auto-incrementing sequence in each shard to define PhotoID because we need to know PhotoID first to find the shard where it will be stored. One solution could be that we dedicate a separate database instance to generate auto-incrementing IDs. If our PhotoID can fit into 64 bits, we can define a table containing only a 64 bit ID field. So whenever we would like to add a photo in our system, we can insert a new row in this table and take that ID to be our PhotoID of the new photo.</p>
<p>Wouldn’t this key generating DB be a single point of failure? Yes, it would be. A workaround for that could be defining two such databases with one generating even numbered IDs and the other odd numbered. For the MySQL, the following script can define such sequences:</p>
<pre><code>KeyGeneratingServer1:
auto-increment-increment = 2
auto-increment-offset = 1 
KeyGeneratingServer2:
auto-increment-increment = 2
auto-increment-offset = 2
</code></pre>
<p>We can put a load balancer in front of both of these databases to round robin between them and to deal with downtime. Both these servers could be out of sync with one generating more keys than the other, but this will not cause any issue in our system. We can extend this design by defining separate ID tables for Users, Photo-Comments, or other objects present in our system.</p>
<p>Alternately, we can implement a ‘key’ generation scheme similar to what we have discussed in Designing a URL Shortening service like TinyURL.</p>
<figure data-type="image" tabindex="2"><img src="https://ywang412.github.io/post-images/1565757623378.png" alt=""></figure>
<p><strong>Design Dropbox</strong></p>
<p>The metadata Database should be storing information about following objects:</p>
<p>Chunks<br>
Files<br>
User<br>
Devices<br>
Workspace (sync folders)</p>
<p><img src="https://ywang412.github.io/post-images/1565757709181.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1565757739488.png" alt=""></p>
<p><strong>Designing Messenger</strong></p>
<p><img src="https://ywang412.github.io/post-images/1566064813925.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1566064820748.png" alt=""></p>
<p><strong>Designing Twitter</strong></p>
<p><img src="https://ywang412.github.io/post-images/1566067729831.png" alt=""><br>
<img src="https://ywang412.github.io/post-images/1566067734387.png" alt=""></p>
<p><strong>Designing Youtube</strong></p>
<p>At a high-level we would need the following components:</p>
<p>Processing Queue: Each uploaded video will be pushed to a processing queue to be de-queued later for encoding, thumbnail generation, and storage.<br>
Encoder: To encode each uploaded video into multiple formats.<br>
Thumbnails generator: To generate a few thumbnails for each video.<br>
Video and Thumbnail storage: To store video and thumbnail files in some distributed file storage.<br>
User Database: To store user’s information, e.g., name, email, address, etc.<br>
Video metadata storage: A metadata database to store all the information about videos like title, file path in the system, uploading user, total views, likes, dislikes, etc. It will also be used to store all the video comments.</p>
<figure data-type="image" tabindex="3"><img src="https://ywang412.github.io/post-images/1566070090262.png" alt=""></figure>
<p><strong>Design Rate Limiter</strong></p>
<figure data-type="image" tabindex="4"><img src="https://ywang412.github.io/post-images/1566091665748.png" alt=""></figure>
<p>fixed window<br>
<img src="https://ywang412.github.io/post-images/1566091776264.png" alt=""><br>
sliding wondow<br>
<img src="https://ywang412.github.io/post-images/1566091784751.png" alt=""><br>
bucket counter<br>
if we have an hourly rate limit we can keep a count for each minute and calculate the sum of all counters in the past hour when we receive a new request to calculate the throttling limit</p>
<p><strong>Design Typeahead Suggestion</strong></p>
<p>Should we have case insensitive trie? For simplicity and search use-case, let’s assume our data is case insensitive.</p>
<p>How to find top suggestion? Now that we can find all the terms for a given prefix, how can we find the top 10 terms for the given prefix? One simple solution could be to store the count of searches that terminated at each node, e.g., if users have searched about ‘CAPTAIN’ 100 times and ‘CAPTION’ 500 times, we can store this number with the last character of the phrase. Now if the user types ‘CAP’ we know the top most searched word under the prefix ‘CAP’ is ‘CAPTION’. So, to find the top suggestions for a given prefix, we can traverse the sub-tree under it.</p>
<p>Given a prefix, how much time will it take to traverse its sub-tree? Given the amount of data we need to index, we should expect a huge tree. Even traversing a sub-tree would take really long, e.g., the phrase ‘system design interview questions’ is 30 levels deep. Since we have very strict latency requirements we do need to improve the efficiency of our solution.</p>
<p>Can we store top suggestions with each node? This can surely speed up our searches but will require a lot of extra storage. We can store top 10 suggestions at each node that we can return to the user. We have to bear the big increase in our storage capacity to achieve the required efficiency.</p>
<p>We can optimize our storage by storing only references of the terminal nodes rather than storing the entire phrase. To find the suggested terms we need to traverse back using the parent reference from the terminal node. We will also need to store the frequency with each reference to keep track of top suggestions.</p>
<p>How would we build this trie? We can efficiently build our trie bottom up. Each parent node will recursively call all the child nodes to calculate their top suggestions and their counts. Parent nodes will combine top suggestions from all of their children to determine their top suggestions.</p>
<p>How to update the trie? Assuming five billion searches every day, which would give us approximately 60K queries per second. If we try to update our trie for every query it’ll be extremely resource intensive and this can hamper our read requests, too. One solution to handle this could be to update our trie offline after a certain interval.</p>
<p>As the new queries come in we can log them and also track their frequencies. Either we can log every query or do sampling and log every 1000th query. For example, if we don’t want to show a term which is searched for less than 1000 times, it’s safe to log every 1000th searched term.</p>
<p>We can have a Map-Reduce (MR) set-up to process all the logging data periodically say every hour. These MR jobs will calculate frequencies of all searched terms in the past hour. We can then update our trie with this new data. We can take the current snapshot of the trie and update it with all the new terms and their frequencies. We should do this offline as we don’t want our read queries to be blocked by update trie requests. We can have two options:</p>
<p>We can make a copy of the trie on each server to update it offline. Once done we can switch to start using it and discard the old one.<br>
Another option is we can have a master-slave configuration for each trie server. We can update slave while the master is serving traffic. Once the update is complete, we can make the slave our new master. We can later update our old master, which can then start serving traffic, too.<br>
How can we update the frequencies of typeahead suggestions? Since we are storing frequencies of our typeahead suggestions with each node, we need to update them too! We can update only differences in frequencies rather than recounting all search terms from scratch. If we’re keeping count of all the terms searched in last 10 days, we’ll need to subtract the counts from the time period no longer included and add the counts for the new time period being included. We can add and subtract frequencies based on Exponential Moving Average (EMA) of each term. In EMA, we give more weight to the latest data. It’s also known as the exponentially weighted moving average.</p>
<p>After inserting a new term in the trie, we’ll go to the terminal node of the phrase and increase its frequency. Since we’re storing the top 10 queries in each node, it is possible that this particular search term jumped into the top 10 queries of a few other nodes. So, we need to update the top 10 queries of those nodes then. We have to traverse back from the node to all the way up to the root. For every parent, we check if the current query is part of the top 10. If so, we update the corresponding frequency. If not, we check if the current query’s frequency is high enough to be a part of the top 10. If so, we insert this new term and remove the term with the lowest frequency.</p>
<p>How can we remove a term from the trie? Let's say we have to remove a term from the trie because of some legal issue or hate or piracy etc. We can completely remove such terms from the trie when the regular update happens, meanwhile, we can add a filtering layer on each server which will remove any such term before sending them to users.</p>
<p>What could be different ranking criteria for suggestions? In addition to a simple count, for terms ranking, we have to consider other factors too, e.g., freshness, user location, language, demographics, personal history etc.</p>
<p>Typeahead Client<br>
We can perform the following optimizations on the client side to improve user’s experience:</p>
<p>The client should only try hitting the server if the user has not pressed any key for 50ms.<br>
If the user is constantly typing, the client can cancel the in-progress requests.<br>
Initially, the client can wait until the user enters a couple of characters.<br>
Clients can pre-fetch some data from the server to save future requests.<br>
Clients can store the recent history of suggestions locally. Recent history has a very high rate of being reused.<br>
Establishing an early connection with the server turns out to be one of the most important factors. As soon as the user opens the search engine website, the client can open a connection with the server. So when a user types in the first character, the client doesn’t waste time in establishing the connection.<br>
The server can push some part of their cache to CDNs and Internet Service Providers (ISPs) for efficiency.</p>
<p><strong>Designing Twitter Search</strong></p>
<figure data-type="image" tabindex="5"><img src="https://ywang412.github.io/post-images/1566094473812.png" alt=""></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java JUC (java.util.concurrent) 7 - ExceptionSemaphore vs Lock Example]]></title>
        <id>https://ywang412.github.io/post/semaphore-vs-lock</id>
        <link href="https://ywang412.github.io/post/semaphore-vs-lock">
        </link>
        <updated>2019-07-14T22:36:52.000Z</updated>
        <content type="html"><![CDATA[<p><strong>1114. Print in Order</strong></p>
<pre><code>class Foo {
    
    private volatile boolean onePrinted;
    private volatile boolean twoPrinted;

    public Foo() {
        onePrinted = false;
        twoPrinted = false;        
    }

    public synchronized void first(Runnable printFirst) throws InterruptedException {
        
        // printFirst.run() outputs &quot;first&quot;. Do not change or remove this line.
        printFirst.run();
        onePrinted = true;
        notifyAll();
    }

    public synchronized void second(Runnable printSecond) throws InterruptedException {
        while(!onePrinted) {
            wait();
        }
        // printSecond.run() outputs &quot;second&quot;. Do not change or remove this line.
        printSecond.run();
        twoPrinted = true;
        notifyAll();
    }

    public synchronized void third(Runnable printThird) throws InterruptedException {
        while(!twoPrinted) {
            wait();
        }
        // printThird.run() outputs &quot;third&quot;. Do not change or remove this line.
        printThird.run();
    }
}
</code></pre>
<pre><code>import java.util.concurrent.*;
class Foo {
    Semaphore run2, run3;

    public Foo() {
        run2 = new Semaphore(0);
        run3 = new Semaphore(0);
    }

    public void first(Runnable printFirst) throws InterruptedException {
        printFirst.run();
        run2.release();
    }

    public void second(Runnable printSecond) throws InterruptedException {
        run2.acquire();
        printSecond.run();
        run3.release();
    }

    public void third(Runnable printThird) throws InterruptedException {
        run3.acquire(); 
        printThird.run();
    }
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java JUC (java.util.concurrent) 6 - Exception]]></title>
        <id>https://ywang412.github.io/post/java-juc-javautilconcurrent-5-exception</id>
        <link href="https://ywang412.github.io/post/java-juc-javautilconcurrent-5-exception">
        </link>
        <updated>2019-07-12T01:45:19.000Z</updated>
        <content type="html"><![CDATA[<p>To stop a thread</p>
<ol>
<li>stop()  // deprecated<br>
throws ThreadDeath exception and release lock</li>
<li>interrupt()   // isInterrupted()</li>
</ol>
<pre><code>public class MyThread extends Thread {
    @Override
    public void run() {
        try {
            for (int i=0; i&lt;50000; i++){
                if (this.isInterrupted()) {
                    System.out.println(&quot; 已经是停止状态了！&quot;);
                    throw new InterruptedException();   // or intead use return;
                }
                System.out.println(i);
            }
            System.out.println(&quot; 不抛出异常，我会被执行的哦！&quot;);
        } catch (Exception e) {
//            e.printStackTrace();
        }
    }
 
    public static void main(String[] args) throws InterruptedException {
        MyThread myThread =new MyThread();
        myThread.start();
        Thread.sleep(100);
        myThread.interrupt();
    }
 
}
</code></pre>
<p>suspend() and resume() // deprecated because occupy lock</p>
<p>suspend can block println()</p>
<pre><code>public void println(String x) {
		synchronized (this) {
				print(x);
				newLine();
		}
}
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://ywang412.github.io/post-images/1572150142349.jpg" alt=""></figure>
<p><strong>uncaughtException</strong></p>
<pre><code>ThreadGroup group = new ThreadGroup(&quot;&quot;){
      @Override
      public void uncaughtException(Thread t, Throwable e) {
             super.uncaughtException(t, e);
             // 一个线程出现异常，中断组内所有线程
             this.interrupt();
      }
};
</code></pre>
<pre><code>public class MyThread{
 
    public static void main(String[] args) {
        ThreadGroup threadGroup = new ThreadGroup(&quot;ThreadGroup&quot;){
            @Override
            public void uncaughtException(Thread t, Throwable e) {
                System.out.println(&quot; 线程组的异常处理 &quot;);
                super.uncaughtException(t, e);
            }
        };
        Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
            @Override
            public void uncaughtException(Thread t, Throwable e) {
                System.out.println(&quot; 线程类的异常处理 &quot;);
            }
        });
        Thread thread = new Thread(threadGroup,&quot;Thread&quot;){
            @Override
            public void run() {
                System.out.println(Thread.currentThread().getName()+&quot; 执行 &quot;);
                int i= 2/0;
            }
        };
//        thread.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
//            @Override
//            public void uncaughtException(Thread t, Throwable e) {
//                System.out.println(&quot; 线程对象的异常处理 &quot;);
//            }
//        });
        thread.start();
    }
 
}

//Thread 执行
//线程组的异常处理
//线程类的异常处理

</code></pre>
]]></content>
    </entry>
</feed>